{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ca0c72-d4dc-42e3-86b2-6301a9da9bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp deepreviewer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d32ed9c-131b-4285-8c25-c93197e34105",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6495c4e4-ba32-4ace-84f9-586d573da54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import os, tiktoken, numpy as np, multiprocessing as mp, pandas as pd, json\n",
    "from datasets import load_dataset\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9fd706-9eed-4d26-9f71-1dfc27495867",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def write_datafile(fname, tokens):\n",
    "    np.save(fname, tokens)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a507698c-1b13-49ad-a787-31bb5bb1b3bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5454e1e-76a4-491f-b102-5a71ba4e117a",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = '/home/scai/phd/aiz218323/scratch/datasets/deepreviewer/train.csv'\n",
    "df = pd.read_csv(filename)\n",
    "\n",
    "docs = [json.loads(df.iloc[i, 0])[1]['content'] for i in range(df.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4ff598-1b4e-4502-8320-b965ad9a8895",
   "metadata": {},
   "outputs": [],
   "source": [
    "shard_size = int(1e8)\n",
    "\n",
    "enc = tiktoken.get_encoding('gpt2')\n",
    "eot = enc._special_tokens['<|endoftext|>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca8466b-aab8-4473-8c08-34ab1e18a797",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(doc):\n",
    "    tokens = [eot]\n",
    "    tokens.extend(enc.encode_ordinary(doc))\n",
    "    tokens_np = np.array(tokens)\n",
    "    assert (0 <= tokens_np).all() and (tokens_np < 2**16).all(), 'token dictionary too large  for uint16'\n",
    "    tokens_np_uint16 = tokens_np.astype(np.uint16)\n",
    "    return tokens_np_uint16\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f2a30c-0402-4e7e-85b6-6db4f5c23ac5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([50256,    59,  7839, ...,   628,   628,   198], dtype=uint16)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o = tokenize(docs[0]); o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea420ea-f2e1-4b7a-8755-48b9eab9d13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_dir = '/home/scai/phd/aiz218323/scratch/datasets/deepreviewer/cache'\n",
    "os.makedirs(cache_dir, exist_ok=True)\n",
    "\n",
    "n_procs = max(1, os.cpu_count()//2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902fd72f-36b7-4e6c-bfb1-ab375767fdf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with mp.Pool(n_procs) as pool:\n",
    "    shard_index,token_count = 0,0\n",
    "    all_tokens_np = np.empty(shard_size, dtype=np.uint16)\n",
    "    \n",
    "    progress_bar = None\n",
    "    for tokens in pool.imap(tokenize, docs, chunksize=16):\n",
    "\n",
    "        if token_count + len(tokens) > shard_size:\n",
    "            all_tokens_np[token_count:token_count+len(tokens)] = tokens\n",
    "            token_count += len(tokens)\n",
    "            if progress_bar is None:\n",
    "                progress_bar = tqdm(total=shard_size, unit=\"tokens\", desc=f\"Shard {shard_index}\")\n",
    "            progress_bar.update(len(tokens))\n",
    "        else:\n",
    "            split = \"val\" if shard_index == 0 else \"train\"\n",
    "            fname = os.path.join(cache_dir, f\"deepreviewer_{split}_{shard_index:06d}\")\n",
    "            \n",
    "            remainder = shard_size - token_count\n",
    "            all_tokens_np[token_count:] = tokens[:remainder]\n",
    "            progress_bar.update(remainder)\n",
    "\n",
    "            \n",
    "            write_datafile(fname, all_tokens_np)\n",
    "            shard_index += 1\n",
    "            progress_bar = None\n",
    "\n",
    "            token_count = len(tokens) - remainder\n",
    "            all_tokens_np[:token_count] = tokens[remainder:]\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201a7c18-8e45-4925-b3e6-1413d60510d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if token_count != 0:\n",
    "    split = \"val\" if shard_index == 0 else \"train\"\n",
    "    fname = os.path.join(cache_dir, f\"deepreviewer_{split}_{shard_index:06d}\")\n",
    "    write_datafile(fname, all_tokens_npp[:token_count])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa730e7-9fa8-4bbb-96c7-ef372454c3b7",
   "metadata": {},
   "source": [
    "## `__main__`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d165e0d-53a9-4de5-bbf4-fd6f701557d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--filename', type=str, required=True)\n",
    "    parser.add_argument('--cache_dir', type=str, required=True)\n",
    "    return parser.parse_args()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23b8573-dabc-411a-a260-60b1d03e7d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "if __name__ == \"__main__\":\n",
    "    args = parse_args()\n",
    "\n",
    "    df = pd.read_csv(args.filename)\n",
    "    docs = [json.loads(df.iloc[i, 0])[1]['content'] for i in range(df.shape[0])]\n",
    "\n",
    "    shard_size = int(1e8)\n",
    "\n",
    "    enc = tiktoken.get_encoding('gpt2')\n",
    "    eot = enc._special_tokens['<|endoftext|>']\n",
    "\n",
    "    def tokenize(doc):\n",
    "        tokens = [eot]\n",
    "        tokens.extend(enc.encode_ordinary(doc))\n",
    "        tokens_np = np.array(tokens)\n",
    "        assert (0 <= tokens_np).all() and (tokens_np < 2**16).all(), 'token dictionary too large  for uint16'\n",
    "        tokens_np_uint16 = tokens_np.astype(np.uint16)\n",
    "        return tokens_np_uint16\n",
    "\n",
    "    os.makedirs(args.cache_dir, exist_ok=True)\n",
    "\n",
    "    n_procs = max(1, os.cpu_count()//2)\n",
    "    with mp.Pool(n_procs) as pool:\n",
    "        shard_index,token_count = 0,0\n",
    "        all_tokens_np = np.empty(shard_size, dtype=np.uint16)\n",
    "        \n",
    "        progress_bar = None\n",
    "        for tokens in pool.imap(tokenize, docs, chunksize=16):\n",
    "    \n",
    "            if token_count + len(tokens) > shard_size:\n",
    "                all_tokens_np[token_count:token_count+len(tokens)] = tokens\n",
    "                token_count += len(tokens)\n",
    "                if progress_bar is None:\n",
    "                    progress_bar = tqdm(total=shard_size, unit=\"tokens\", desc=f\"Shard {shard_index}\")\n",
    "                progress_bar.update(len(tokens))\n",
    "            else:\n",
    "                split = \"val\" if shard_index == 0 else \"train\"\n",
    "                fname = os.path.join(args.cache_dir, f\"deepreviewer_{split}_{shard_index:06d}\")\n",
    "                \n",
    "                remainder = shard_size - token_count\n",
    "                all_tokens_np[token_count:] = tokens[:remainder]\n",
    "                progress_bar.update(remainder)\n",
    "    \n",
    "                \n",
    "                write_datafile(fname, all_tokens_np)\n",
    "                shard_index += 1\n",
    "                progress_bar = None\n",
    "    \n",
    "                token_count = len(tokens) - remainder\n",
    "                all_tokens_np[:token_count] = tokens[remainder:]\n",
    "\n",
    "    if token_count != 0:\n",
    "        split = \"val\" if shard_index == 0 else \"train\"\n",
    "        fname = os.path.join(args.cache_dir, f\"deepreviewer_{split}_{shard_index:06d}\")\n",
    "        write_datafile(fname, all_tokens_npp[:token_count])\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
