{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b983753a-0042-4422-9162-5feefe06cae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ead903-6ccf-4e61-aa0c-f9eda576a8de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0155b7d4-d256-4b01-9867-105d1f158940",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4f3e37-aff2-4d50-95ed-c286f2b8690a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import torch.distributed as dist, torch, os, tiktoken, numpy as np, time\n",
    "from torch.distributed import init_process_group, destroy_process_group\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "\n",
    "from scratchRLHF.models.gpt2 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b13504-fe19-4759-bd9c-5545f36f7138",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def load_tokens(filename):\n",
    "    npt = np.load(filename).astype(np.int64)\n",
    "    ptt = torch.tensor(npt, dtype=torch.long)\n",
    "    return ptt\n",
    "\n",
    "class DataLoaderLite:\n",
    "\n",
    "    def __init__(self, B, T, process_rank, num_processes, split):\n",
    "        self.B, self.T = B, T\n",
    "        self.process_rank = process_rank\n",
    "        self.num_processes = num_processes\n",
    "        assert split in {'train', 'val'}\n",
    "\n",
    "        # get shard files\n",
    "        data_root = \"/scratch/scai/phd/aiz218323/datasets/deepreviewer/cache/\"\n",
    "        shards = os.listdir(data_root)\n",
    "        shards = [s for s in shards if split in s]\n",
    "        shards = sorted(shards)\n",
    "        shards = [os.path.join(data_root, s) for s in shards]\n",
    "        self.shards = shards\n",
    "        assert len(shards) > 0, f\"no shard found for split {split}\"\n",
    "        if master_process:\n",
    "            print(f'found {len(shards)} shards for split {split}')\n",
    "\n",
    "        # state, init at shard zero\n",
    "        self.current_shard = 0\n",
    "        self.tokens = load_tokens(self.shards[self.current_shard])\n",
    "        self.current_position = self.B * self.T * self.process_rank\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_shard = 0\n",
    "        self.tokens = load_tokens(self.shards[self.current_shard])\n",
    "        self.current_position = self.B * self.T * self.process_rank\n",
    "\n",
    "    def next_batch(self):\n",
    "        B, T = self.B, self.T\n",
    "        buf = self.tokens[self.current_position:self.current_position+(B*T)+1]\n",
    "        x = buf[:-1].view(B, T)\n",
    "        y = buf[1:].view(B, T)\n",
    "\n",
    "        self.current_position += (B * T * self.num_processes)\n",
    "        if self.current_position + (B * T * self.num_processes + 1) > len(self.tokens):\n",
    "            self.current_shard = (self.current_shard + 1) % len(self.shards)\n",
    "            self.tokens = load_tokens(self.shards[self.current_shard])\n",
    "            self.current_position = self.B * self.T * self.process_rank\n",
    "        return x, y\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb78bf49-32e6-40e7-b52e-f051ff97e696",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device : cpu\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "ddp = int(os.environ.get('RANK', -1)) != -1\n",
    "\n",
    "if ddp:\n",
    "    assert torch.cuda.is_available(), \"CUDA is required for DDP\"\n",
    "    init_process_group(backend='nccl')\n",
    "    ddp_rank = int(os.environ['RANK'])\n",
    "    ddp_local_rank = int(os.environ['LOCAL_RANK'])\n",
    "    ddp_world_size = int(os.environ['WORLD_SIZE'])\n",
    "    device = f'cuda:{ddp_local_rank}'\n",
    "    torch.cuda.set_device(device)\n",
    "    master_process = ddp_rank == 0\n",
    "else:\n",
    "    ddp_rank = 0\n",
    "    ddp_local_rank = 0\n",
    "    ddp_world_size = 1\n",
    "    master_process = True\n",
    "    device = 'cpu'\n",
    "    if torch.cuda.is_available():\n",
    "        device = 'cuda'\n",
    "    elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "        device = 'mps'\n",
    "    print(f'Using device : {device}')\n",
    "\n",
    "device_type = \"cuda\" if device.startswith(\"cuda\") else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2affd2a9-a380-416a-9dbe-7f4019121b50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45029c2-d298-4dd2-8121-dddf7927f317",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total desired batch size: 524288\n",
      "=> calculated gradient accumulation steps: 64\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "total_batch_size = 524288\n",
    "B = 8\n",
    "T = 1024\n",
    "\n",
    "assert total_batch_size % (B * T * ddp_world_size) == 0, \"make sure total_batch_size is divisible by B * T * ddp_world_size\"\n",
    "\n",
    "grad_accum_steps = total_batch_size // (B * T * ddp_world_size)\n",
    "if master_process:\n",
    "    print(f'total desired batch size: {total_batch_size}')\n",
    "    print(f'=> calculated gradient accumulation steps: {grad_accum_steps}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d65796-4b3e-432c-9996-950f7f5faeb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading weights from pretrained gpt: gpt2\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "model_path = '/home/scai/phd/aiz218323/scratch/outputs/scratchRLHF/gpt2.pth'\n",
    "\n",
    "model = GPT.from_pretrained('gpt2', model_path)\n",
    "model = model.to(device)\n",
    "if ddp:\n",
    "    model = DDP(model, device_ids=[ddp_local_rank])\n",
    "raw_model = model.module if ddp else model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63f516b-0478-40ff-8e03-26a62c69d890",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "model = torch.compile(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bfe485a-0592-4e3d-8f4f-9ec324f0d864",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 4 shards for split train\n",
      "found 1 shards for split val\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "train_dataloader = DataLoaderLite(B=8, T=1024, process_rank=ddp_rank, num_processes=ddp_world_size, split='train')\n",
    "val_dataloader = DataLoaderLite(B=8, T=1024, process_rank=ddp_rank, num_processes=ddp_world_size, split='val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7574d4e0-306f-4413-b4bf-eeeb64a1b3a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3a6278-31c4-43d5-b849-84fc2d0363ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "max_lr = 6e-4\n",
    "min_lr = max_lr * 0.1\n",
    "\n",
    "# warmup_steps = 10\n",
    "# max_steps = 50\n",
    "\n",
    "warmup_steps = 715\n",
    "max_steps = 19073\n",
    "\n",
    "save_steps = 1000\n",
    "model_dir = \"/home/scai/phd/aiz218323/scratch/outputs/scratchRLHF/models\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "def get_lr(it):\n",
    "    if it < warmup_steps:\n",
    "        return max_lr * (it + 1) / warmup_steps\n",
    "\n",
    "    if it > max_steps:\n",
    "        return min_lr\n",
    "\n",
    "    decay_ratio = (it - warmup_steps)/(max_steps - warmup_steps)\n",
    "    assert 0 <= decay_ratio <= 1\n",
    "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))\n",
    "    return min_lr + coeff * (max_lr - min_lr)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d84b73-e7a7-47ec-9b2b-a7a8bd7ebd7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "log_dir = \"/home/scai/phd/aiz218323/scratch/outputs/scratchRLHF/log\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "log_file = os.path.join(log_dir, 'log.txt')\n",
    "with open(log_file, 'w') as f:\n",
    "    pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b9345d-de75-49d7-86f9-ff5e7997fe5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "optimizer = raw_model.configure_optimizers(weight_decay=0.1, learning_rate=6e-4, device=device)\n",
    "\n",
    "for step in range(max_steps):\n",
    "    t0 = time.time()\n",
    "    last_step = (step == max_steps - 1)\n",
    "\n",
    "    if step % 100 == 0 or last_step:\n",
    "        model.eval()\n",
    "        val_dataloader.reset()\n",
    "        with torch.no_grad():\n",
    "            val_loss_accum = 0.0\n",
    "            val_loss_steps = 20\n",
    "            for _ in range(val_loss_steps):\n",
    "                x, y = val_dataloader.next_batch()\n",
    "                x, y = x.to(device), y.to(device)\n",
    "                with torch.autocast(device_type=device_type, dtype=torch.bfloat16):\n",
    "                    logits, loss = model(x, y)\n",
    "                loss = loss / val_loss_steps\n",
    "                val_loss_accum += loss.detach()\n",
    "        if ddp:\n",
    "            dist.all_reduce(val_loss_accum, op=dist.ReduceOp.AVG)\n",
    "        if master_process:\n",
    "            print(f'validation loss: {val_loss_accum.item():.4f}')\n",
    "            with open(log_file, 'w') as f:\n",
    "                f.write(f'{step} val {val_loss_accum.item():.4f}\\n')\n",
    "                \n",
    "        if step % 1000 == 0 or last_step:\n",
    "            raw_model.save(f\"{model_dir}/gpt2_{step:05d}.pth\")\n",
    "\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    loss_accum = 0.0\n",
    "    for micro_step in range(grad_accum_steps):\n",
    "        x, y = train_dataloader.next_batch()\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        logits, loss = model(x, y)\n",
    "        loss = loss / grad_accum_steps\n",
    "        loss_accum += loss.detach()\n",
    "        if ddp:\n",
    "            model.require_backward_grad_sync = (micro_step == grad_accum_steps - 1)\n",
    "        loss.backward()\n",
    "    if ddp:\n",
    "        dist.all_reduce(loss_accum, op=dist.ReduceOp.AVG)\n",
    "    norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "    lr = get_lr(step)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "    \n",
    "    optimizer.step()\n",
    "    torch.cuda.synchronize()\n",
    "    \n",
    "    t1 = time.time()\n",
    "    dt = (t1 - t0) * 1000\n",
    "    tokens_per_sec = (train_dataloader.B * train_dataloader.T * grad_accum_steps * ddp_world_size)/(t1 - t0)\n",
    "    print(f\"Step {step} | loss: {loss_accum.item()} | lr: {lr:.4e} norm: {norm:.4f} | dt: {dt:.2f}ms | tok/sec: {tokens_per_sec:.2f}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a7e937-b472-4b69-a173-92b11e301273",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "if ddp:\n",
    "    destroy_process_group()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dcc13f8-3e43-47ad-a350-626220d64468",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
